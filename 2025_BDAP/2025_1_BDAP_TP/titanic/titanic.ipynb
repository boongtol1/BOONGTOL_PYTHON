{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b1f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_14224/3248462934.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_14224/3248462934.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_14224/3248462934.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_14224/3248462934.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_14224/3248462934.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_14224/3248462934.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
      "/opt/miniconda3/envs/VDLP/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-05-30 19:59:57.523326: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2025-05-30 19:59:57.523359: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 96.00 GB\n",
      "2025-05-30 19:59:57.523364: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 36.00 GB\n",
      "2025-05-30 19:59:57.523382: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-30 19:59:57.523397: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 19:59:58.174591: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.4622 - loss: 0.8758 - val_accuracy: 0.6238 - val_loss: 0.7384\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6283 - loss: 0.7551 - val_accuracy: 0.7000 - val_loss: 0.6402\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7030 - loss: 0.6310 - val_accuracy: 0.7286 - val_loss: 0.5837\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7131 - loss: 0.5993 - val_accuracy: 0.7429 - val_loss: 0.5433\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7281 - loss: 0.5531 - val_accuracy: 0.7619 - val_loss: 0.5140\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7375 - loss: 0.5388 - val_accuracy: 0.8000 - val_loss: 0.4881\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7400 - loss: 0.5214 - val_accuracy: 0.8143 - val_loss: 0.4674\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7807 - loss: 0.4998 - val_accuracy: 0.8429 - val_loss: 0.4488\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8152 - loss: 0.4608 - val_accuracy: 0.8714 - val_loss: 0.4351\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8109 - loss: 0.4675 - val_accuracy: 0.8762 - val_loss: 0.4244\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8224 - loss: 0.4390 - val_accuracy: 0.8667 - val_loss: 0.4132\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8056 - loss: 0.4414 - val_accuracy: 0.8714 - val_loss: 0.4044\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8260 - loss: 0.4377 - val_accuracy: 0.8667 - val_loss: 0.3963\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8322 - loss: 0.4110 - val_accuracy: 0.8762 - val_loss: 0.3904\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8395 - loss: 0.3997 - val_accuracy: 0.8762 - val_loss: 0.3840\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8238 - loss: 0.4669 - val_accuracy: 0.8762 - val_loss: 0.3822\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8378 - loss: 0.4296 - val_accuracy: 0.8714 - val_loss: 0.3813\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8367 - loss: 0.4094 - val_accuracy: 0.8714 - val_loss: 0.3787\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8609 - loss: 0.3906 - val_accuracy: 0.8714 - val_loss: 0.3740\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8282 - loss: 0.4328 - val_accuracy: 0.8714 - val_loss: 0.3732\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8426 - loss: 0.3841 - val_accuracy: 0.8714 - val_loss: 0.3698\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8269 - loss: 0.4372 - val_accuracy: 0.8714 - val_loss: 0.3708\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8429 - loss: 0.4000 - val_accuracy: 0.8714 - val_loss: 0.3717\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8283 - loss: 0.4408 - val_accuracy: 0.8667 - val_loss: 0.3730\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8353 - loss: 0.4196 - val_accuracy: 0.8714 - val_loss: 0.3722\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8345 - loss: 0.4046 - val_accuracy: 0.8714 - val_loss: 0.3691\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8368 - loss: 0.4265 - val_accuracy: 0.8714 - val_loss: 0.3680\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8714 - loss: 0.3523 - val_accuracy: 0.8714 - val_loss: 0.3652\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8249 - loss: 0.4435 - val_accuracy: 0.8667 - val_loss: 0.3688\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8583 - loss: 0.3670 - val_accuracy: 0.8714 - val_loss: 0.3657\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8207 - loss: 0.4272 - val_accuracy: 0.8714 - val_loss: 0.3635\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8295 - loss: 0.3989 - val_accuracy: 0.8714 - val_loss: 0.3640\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8359 - loss: 0.4203 - val_accuracy: 0.8762 - val_loss: 0.3627\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8794 - loss: 0.3450 - val_accuracy: 0.8714 - val_loss: 0.3637\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8653 - loss: 0.3909 - val_accuracy: 0.8714 - val_loss: 0.3657\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8787 - loss: 0.3415 - val_accuracy: 0.8714 - val_loss: 0.3654\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8436 - loss: 0.3801 - val_accuracy: 0.8667 - val_loss: 0.3677\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8570 - loss: 0.3652 - val_accuracy: 0.8714 - val_loss: 0.3653\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8416 - loss: 0.4088 - val_accuracy: 0.8714 - val_loss: 0.3651\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8401 - loss: 0.4145 - val_accuracy: 0.8714 - val_loss: 0.3652\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8424 - loss: 0.3980 - val_accuracy: 0.8762 - val_loss: 0.3646\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8622 - loss: 0.3786 - val_accuracy: 0.8762 - val_loss: 0.3641\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8659 - loss: 0.3697 - val_accuracy: 0.8762 - val_loss: 0.3631\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8546 - loss: 0.3914 - val_accuracy: 0.8762 - val_loss: 0.3649\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8377 - loss: 0.4130 - val_accuracy: 0.8714 - val_loss: 0.3641\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8387 - loss: 0.4119 - val_accuracy: 0.8714 - val_loss: 0.3627\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8594 - loss: 0.3840 - val_accuracy: 0.8714 - val_loss: 0.3643\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8588 - loss: 0.3772 - val_accuracy: 0.8714 - val_loss: 0.3637\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8556 - loss: 0.3907 - val_accuracy: 0.8714 - val_loss: 0.3655\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8427 - loss: 0.4092 - val_accuracy: 0.8762 - val_loss: 0.3656\n",
      "[TensorFlow] Test Accuracy: 0.8473\n"
     ]
    }
   ],
   "source": [
    "# titanic_tf_simple.py\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1️⃣ 데이터 불러오기\n",
    "df = pd.read_csv(\"titanic1309.csv\")\n",
    "\n",
    "# 2️⃣ 사용할 열 선택\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = df['Survived']\n",
    "\n",
    "# 3️⃣ 결측치 처리 (수치형: 평균으로, 범주형: 가장 많은 값으로)\n",
    "X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
    "X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
    "X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# 4️⃣ 범주형을 숫자로 변환 (원-핫 인코딩)\n",
    "X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# 5️⃣ 수치형 변수 스케일링 (평균=0, 표준편차=1)\n",
    "scaler = StandardScaler()\n",
    "X[['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']] = scaler.fit_transform(X[['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']])\n",
    "\n",
    "# 6️⃣ 학습용, 테스트용 데이터셋으로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7️⃣ TensorFlow 모델\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 8️⃣ 테스트셋 정확도 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"[TensorFlow] Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3064872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae8609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/749973377.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/749973377.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/749973377.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/749973377.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/749973377.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/749973377.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/VDLP/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-01 11:03:14.146957: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2025-06-01 11:03:14.146997: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 96.00 GB\n",
      "2025-06-01 11:03:14.147002: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 36.00 GB\n",
      "2025-06-01 11:03:14.147155: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-01 11:03:14.147167: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-06-01 11:03:14.521727: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6999 - loss: 0.6038 - val_accuracy: 0.7810 - val_loss: 0.5012\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7187 - loss: 0.5590 - val_accuracy: 0.8286 - val_loss: 0.4718\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7403 - loss: 0.5412 - val_accuracy: 0.8381 - val_loss: 0.4560\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7449 - loss: 0.5484 - val_accuracy: 0.8571 - val_loss: 0.4453\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7725 - loss: 0.5490 - val_accuracy: 0.8619 - val_loss: 0.4306\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7792 - loss: 0.4889 - val_accuracy: 0.8762 - val_loss: 0.4187\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8069 - loss: 0.4843 - val_accuracy: 0.8619 - val_loss: 0.4119\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7806 - loss: 0.4980 - val_accuracy: 0.8714 - val_loss: 0.4079\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8134 - loss: 0.4649 - val_accuracy: 0.8810 - val_loss: 0.3989\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8155 - loss: 0.4560 - val_accuracy: 0.8762 - val_loss: 0.3948\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8136 - loss: 0.4454 - val_accuracy: 0.8714 - val_loss: 0.3868\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8058 - loss: 0.4718 - val_accuracy: 0.8762 - val_loss: 0.3841\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8252 - loss: 0.4398 - val_accuracy: 0.8714 - val_loss: 0.3831\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8213 - loss: 0.4124 - val_accuracy: 0.8762 - val_loss: 0.3793\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8145 - loss: 0.4471 - val_accuracy: 0.8714 - val_loss: 0.3804\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8346 - loss: 0.4067 - val_accuracy: 0.8762 - val_loss: 0.3772\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8100 - loss: 0.4508 - val_accuracy: 0.8762 - val_loss: 0.3752\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8158 - loss: 0.4271 - val_accuracy: 0.8762 - val_loss: 0.3746\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8269 - loss: 0.4063 - val_accuracy: 0.8714 - val_loss: 0.3732\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8238 - loss: 0.4225 - val_accuracy: 0.8762 - val_loss: 0.3711\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8377 - loss: 0.4044 - val_accuracy: 0.8762 - val_loss: 0.3708\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8370 - loss: 0.4177 - val_accuracy: 0.8762 - val_loss: 0.3700\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7963 - loss: 0.4572 - val_accuracy: 0.8857 - val_loss: 0.3714\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8348 - loss: 0.3936 - val_accuracy: 0.8810 - val_loss: 0.3713\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8136 - loss: 0.4238 - val_accuracy: 0.8810 - val_loss: 0.3724\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8542 - loss: 0.4004 - val_accuracy: 0.8810 - val_loss: 0.3703\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8207 - loss: 0.4221 - val_accuracy: 0.8810 - val_loss: 0.3727\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8356 - loss: 0.4090 - val_accuracy: 0.8810 - val_loss: 0.3720\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8535 - loss: 0.3675 - val_accuracy: 0.8762 - val_loss: 0.3708\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8361 - loss: 0.4032 - val_accuracy: 0.8762 - val_loss: 0.3714\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8263 - loss: 0.4052 - val_accuracy: 0.8762 - val_loss: 0.3688\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8443 - loss: 0.4001 - val_accuracy: 0.8762 - val_loss: 0.3700\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8179 - loss: 0.4321 - val_accuracy: 0.8714 - val_loss: 0.3684\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8321 - loss: 0.4136 - val_accuracy: 0.8810 - val_loss: 0.3679\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8398 - loss: 0.4087 - val_accuracy: 0.8810 - val_loss: 0.3701\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8293 - loss: 0.4282 - val_accuracy: 0.8810 - val_loss: 0.3686\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8410 - loss: 0.4269 - val_accuracy: 0.8810 - val_loss: 0.3683\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8331 - loss: 0.4206 - val_accuracy: 0.8857 - val_loss: 0.3698\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8294 - loss: 0.4262 - val_accuracy: 0.8762 - val_loss: 0.3720\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8281 - loss: 0.4317 - val_accuracy: 0.8762 - val_loss: 0.3716\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8376 - loss: 0.4354 - val_accuracy: 0.8762 - val_loss: 0.3714\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8583 - loss: 0.3769 - val_accuracy: 0.8714 - val_loss: 0.3718\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8642 - loss: 0.3751 - val_accuracy: 0.8857 - val_loss: 0.3712\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8336 - loss: 0.4114 - val_accuracy: 0.8857 - val_loss: 0.3715\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8267 - loss: 0.4119 - val_accuracy: 0.8810 - val_loss: 0.3722\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8410 - loss: 0.3930 - val_accuracy: 0.8810 - val_loss: 0.3735\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8495 - loss: 0.3949 - val_accuracy: 0.8810 - val_loss: 0.3735\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8209 - loss: 0.4323 - val_accuracy: 0.8857 - val_loss: 0.3749\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8262 - loss: 0.4203 - val_accuracy: 0.8810 - val_loss: 0.3742\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8132 - loss: 0.4463 - val_accuracy: 0.8810 - val_loss: 0.3730\n",
      "[TensorFlow] Test Accuracy: 0.8435\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      "🎯 각 샘플별 생존 확률 (앞 10개 샘플):\n",
      "샘플 1 → 생존 확률: 0.9403\n",
      "샘플 2 → 생존 확률: 0.0479\n",
      "샘플 3 → 생존 확률: 0.4011\n",
      "샘플 4 → 생존 확률: 0.2822\n",
      "샘플 5 → 생존 확률: 0.3133\n",
      "샘플 6 → 생존 확률: 0.8664\n",
      "샘플 7 → 생존 확률: 0.2642\n",
      "샘플 8 → 생존 확률: 0.0818\n",
      "샘플 9 → 생존 확률: 0.6787\n",
      "샘플 10 → 생존 확률: 0.0689\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1️⃣ 데이터 불러오기\n",
    "df = pd.read_csv(\"titanic1309.csv\")\n",
    "\n",
    "# 2️⃣ 사용할 열 선택\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = df['Survived']\n",
    "\n",
    "# 3️⃣ 결측치 처리 (수치형: 평균으로, 범주형: 가장 많은 값으로)\n",
    "X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
    "X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
    "X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# 4️⃣ 범주형을 숫자로 변환 (원-핫 인코딩)\n",
    "X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# 5️⃣ 수치형 변수 스케일링 (평균=0, 표준편차=1)\n",
    "scaler = StandardScaler()\n",
    "X[['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']] = scaler.fit_transform(X[['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']])\n",
    "\n",
    "# 6️⃣ 학습용, 테스트용 데이터셋으로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7️⃣ TensorFlow 모델\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 8️⃣ 테스트셋 정확도 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"[TensorFlow] Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 9️⃣ 테스트셋 각 샘플별 생존 확률 출력\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# 0~1 사이의 생존 확률 출력 (앞부분만 일부 확인)\n",
    "print(\"\\n🎯 각 샘플별 생존 확률 (앞 10개 샘플):\")\n",
    "for idx, prob in enumerate(y_pred_probs[:10]):\n",
    "    print(f\"샘플 {idx+1} → 생존 확률: {prob[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e3c23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/796972916.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/796972916.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/796972916.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/796972916.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/796972916.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/796972916.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
      "/opt/miniconda3/envs/VDLP/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6455 - loss: 0.6715 - val_accuracy: 0.7571 - val_loss: 0.5885\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6708 - loss: 0.6123 - val_accuracy: 0.7524 - val_loss: 0.5411\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6944 - loss: 0.5752 - val_accuracy: 0.7571 - val_loss: 0.5165\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7261 - loss: 0.5293 - val_accuracy: 0.7667 - val_loss: 0.5013\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7239 - loss: 0.5379 - val_accuracy: 0.7714 - val_loss: 0.4883\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7488 - loss: 0.5099 - val_accuracy: 0.7810 - val_loss: 0.4768\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7403 - loss: 0.5184 - val_accuracy: 0.7905 - val_loss: 0.4655\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7157 - loss: 0.5267 - val_accuracy: 0.8095 - val_loss: 0.4584\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7764 - loss: 0.5036 - val_accuracy: 0.8381 - val_loss: 0.4493\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7968 - loss: 0.4711 - val_accuracy: 0.8333 - val_loss: 0.4409\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7961 - loss: 0.4698 - val_accuracy: 0.8571 - val_loss: 0.4314\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7781 - loss: 0.4778 - val_accuracy: 0.8524 - val_loss: 0.4227\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8061 - loss: 0.4886 - val_accuracy: 0.8571 - val_loss: 0.4146\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8167 - loss: 0.4581 - val_accuracy: 0.8619 - val_loss: 0.4060\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8069 - loss: 0.4213 - val_accuracy: 0.8619 - val_loss: 0.4009\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8037 - loss: 0.4299 - val_accuracy: 0.8524 - val_loss: 0.3944\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8356 - loss: 0.4062 - val_accuracy: 0.8571 - val_loss: 0.3916\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8486 - loss: 0.3901 - val_accuracy: 0.8571 - val_loss: 0.3870\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8392 - loss: 0.3984 - val_accuracy: 0.8571 - val_loss: 0.3844\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8343 - loss: 0.4174 - val_accuracy: 0.8571 - val_loss: 0.3842\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8379 - loss: 0.3752 - val_accuracy: 0.8571 - val_loss: 0.3808\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8378 - loss: 0.3984 - val_accuracy: 0.8571 - val_loss: 0.3780\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8318 - loss: 0.3832 - val_accuracy: 0.8667 - val_loss: 0.3762\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8432 - loss: 0.3963 - val_accuracy: 0.8667 - val_loss: 0.3770\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8347 - loss: 0.4240 - val_accuracy: 0.8667 - val_loss: 0.3772\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8508 - loss: 0.3908 - val_accuracy: 0.8667 - val_loss: 0.3744\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8386 - loss: 0.3870 - val_accuracy: 0.8667 - val_loss: 0.3733\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8278 - loss: 0.4319 - val_accuracy: 0.8667 - val_loss: 0.3726\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8585 - loss: 0.3781 - val_accuracy: 0.8667 - val_loss: 0.3712\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8330 - loss: 0.4079 - val_accuracy: 0.8667 - val_loss: 0.3713\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8580 - loss: 0.3673 - val_accuracy: 0.8667 - val_loss: 0.3715\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8504 - loss: 0.3851 - val_accuracy: 0.8667 - val_loss: 0.3699\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8619 - loss: 0.3735 - val_accuracy: 0.8667 - val_loss: 0.3713\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8706 - loss: 0.3384 - val_accuracy: 0.8667 - val_loss: 0.3711\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8066 - loss: 0.4132 - val_accuracy: 0.8667 - val_loss: 0.3719\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8308 - loss: 0.3916 - val_accuracy: 0.8667 - val_loss: 0.3728\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8474 - loss: 0.3858 - val_accuracy: 0.8667 - val_loss: 0.3705\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8568 - loss: 0.3867 - val_accuracy: 0.8667 - val_loss: 0.3687\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8496 - loss: 0.4026 - val_accuracy: 0.8667 - val_loss: 0.3694\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8367 - loss: 0.3987 - val_accuracy: 0.8762 - val_loss: 0.3684\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8548 - loss: 0.3698 - val_accuracy: 0.8714 - val_loss: 0.3687\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8578 - loss: 0.3920 - val_accuracy: 0.8667 - val_loss: 0.3689\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7930 - loss: 0.4856 - val_accuracy: 0.8667 - val_loss: 0.3689\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8237 - loss: 0.4305 - val_accuracy: 0.8714 - val_loss: 0.3674\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8530 - loss: 0.3739 - val_accuracy: 0.8667 - val_loss: 0.3703\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8240 - loss: 0.4103 - val_accuracy: 0.8667 - val_loss: 0.3692\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8370 - loss: 0.3906 - val_accuracy: 0.8571 - val_loss: 0.3726\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8513 - loss: 0.3839 - val_accuracy: 0.8619 - val_loss: 0.3723\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8354 - loss: 0.4111 - val_accuracy: 0.8714 - val_loss: 0.3687\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8331 - loss: 0.3981 - val_accuracy: 0.8714 - val_loss: 0.3701\n",
      "[TensorFlow] Test Accuracy: 0.8397\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      "🎯 테스트셋 각 샘플별 생존 확률:\n",
      "PassengerId: 951, Name: Chaudanson, Miss. Victorine, 생존 확률: 0.9661\n",
      "PassengerId: 812, Name: Lester, Mr. James, 생존 확률: 0.0538\n",
      "PassengerId: 476, Name: Clifford, Mr. George Quincy, 생존 확률: 0.4435\n",
      "PassengerId: 1137, Name: Kenyon, Mr. Frederick R, 생존 확률: 0.3089\n",
      "PassengerId: 138, Name: Futrelle, Mr. Jacques Heath, 생존 확률: 0.3411\n",
      "PassengerId: 944, Name: Hocking, Miss. Ellen Nellie\"\", 생존 확률: 0.8641\n",
      "PassengerId: 903, Name: Jones, Mr. Charles Cresson, 생존 확률: 0.2911\n",
      "PassengerId: 754, Name: Jonkoff, Mr. Lalio, 생존 확률: 0.0868\n",
      "PassengerId: 1302, Name: Naughton, Miss. Hannah, 생존 확률: 0.7591\n",
      "PassengerId: 354, Name: Arnold-Franchi, Mr. Josef, 생존 확률: 0.0709\n",
      "PassengerId: 1024, Name: Lefebre, Mrs. Frank (Frances), 생존 확률: 0.5870\n",
      "PassengerId: 1230, Name: Denbury, Mr. Herbert, 생존 확률: 0.2256\n",
      "PassengerId: 1241, Name: Walcroft, Miss. Nellie, 생존 확률: 0.8755\n",
      "PassengerId: 28, Name: Fortune, Mr. Charles Alexander, 생존 확률: 0.5159\n",
      "PassengerId: 4, Name: Futrelle, Mrs. Jacques Heath (Lily May Peel), 생존 확률: 0.9451\n",
      "PassengerId: 565, Name: Meanwell, Miss. (Marion Ogden), 생존 확률: 0.6974\n",
      "PassengerId: 161, Name: Cribb, Mr. John Hatfield, 생존 확률: 0.0388\n",
      "PassengerId: 1161, Name: Pokrnic, Mr. Mate, 생존 확률: 0.1056\n",
      "PassengerId: 667, Name: Butler, Mr. Reginald Fenton, 생존 확률: 0.2147\n",
      "PassengerId: 830, Name: Stone, Mrs. George Nelson (Martha Evelyn), 생존 확률: 0.8963\n",
      "PassengerId: 555, Name: Ohman, Miss. Velin, 생존 확률: 0.7531\n",
      "PassengerId: 181, Name: Sage, Miss. Constance Gladys, 생존 확률: 0.3361\n",
      "PassengerId: 1134, Name: Spedden, Mr. Frederic Oakley, 생존 확률: 0.2389\n",
      "PassengerId: 329, Name: Goldsmith, Mrs. Frank John (Emily Alice Brown), 생존 확률: 0.6272\n",
      "PassengerId: 1297, Name: Nourney, Mr. Alfred (Baron von Drachstedt\")\", 생존 확률: 0.1866\n",
      "PassengerId: 933, Name: Franklin, Mr. Thomas Parham, 생존 확률: 0.4222\n",
      "PassengerId: 1005, Name: Buckley, Miss. Katherine, 생존 확률: 0.8252\n",
      "PassengerId: 436, Name: Carter, Miss. Lucile Polk, 생존 확률: 0.9721\n",
      "PassengerId: 647, Name: Cor, Mr. Liudevit, 생존 확률: 0.0988\n",
      "PassengerId: 1142, Name: West, Miss. Barbara J, 생존 확률: 0.9303\n",
      "PassengerId: 153, Name: Meo, Mr. Alfonzo, 생존 확률: 0.0290\n",
      "PassengerId: 246, Name: Minahan, Dr. William Edward, 생존 확률: 0.3429\n",
      "PassengerId: 915, Name: Williams, Mr. Richard Norris II, 생존 확률: 0.4082\n",
      "PassengerId: 1052, Name: Smyth, Miss. Julia, 생존 확률: 0.7591\n",
      "PassengerId: 1073, Name: Compton, Mr. Alexander Taylor Jr, 생존 확률: 0.2594\n",
      "PassengerId: 392, Name: Jansson, Mr. Carl Olof, 생존 확률: 0.0926\n",
      "PassengerId: 1107, Name: Head, Mr. Christopher, 생존 확률: 0.3338\n",
      "PassengerId: 1128, Name: Warren, Mr. Frank Manley, 생존 확률: 0.1299\n",
      "PassengerId: 638, Name: Collyer, Mr. Harvey, 생존 확률: 0.1440\n",
      "PassengerId: 567, Name: Stoytcheff, Mr. Ilia, 생존 확률: 0.0988\n",
      "PassengerId: 516, Name: Walker, Mr. William Anderson, 생존 확률: 0.2894\n",
      "PassengerId: 1251, Name: Lindell, Mrs. Edvard Bengtsson (Elin Gerda Persson), 생존 확률: 0.6625\n",
      "PassengerId: 48, Name: O'Driscoll, Miss. Bridget, 생존 확률: 0.7591\n",
      "PassengerId: 1307, Name: Saether, Mr. Simon Sivertsen, 생존 확률: 0.0518\n",
      "PassengerId: 2, Name: Cumings, Mrs. John Bradley (Florence Briggs Thayer), 생존 확률: 0.9201\n",
      "PassengerId: 1061, Name: Hellstrom, Miss. Hilda Maria, 생존 확률: 0.7538\n",
      "PassengerId: 660, Name: Newell, Mr. Arthur Webster, 생존 확률: 0.1613\n",
      "PassengerId: 643, Name: Skoog, Miss. Margit Elizabeth, 생존 확률: 0.7464\n",
      "PassengerId: 378, Name: Widener, Mr. Harry Elkins, 생존 확률: 0.4480\n",
      "PassengerId: 818, Name: Mallet, Mr. Albert, 생존 확률: 0.1088\n",
      "PassengerId: 195, Name: Brown, Mrs. James Joseph (Margaret Tobin), 생존 확률: 0.9058\n",
      "PassengerId: 129, Name: Peter, Miss. Anna, 생존 확률: 0.5521\n",
      "PassengerId: 902, Name: Ilieff, Mr. Ylio, 생존 확률: 0.0692\n",
      "PassengerId: 16, Name: Hewlett, Mrs. (Mary D Kingcome) , 생존 확률: 0.7461\n",
      "PassengerId: 355, Name: Yousif, Mr. Wazli, 생존 확률: 0.0494\n",
      "PassengerId: 104, Name: Johansson, Mr. Gustaf Joel, 생존 확률: 0.0626\n",
      "PassengerId: 92, Name: Andreasson, Mr. Paul Edvin, 생존 확률: 0.0956\n",
      "PassengerId: 582, Name: Thayer, Mrs. John Borland (Marian Longstreth Morris), 생존 확률: 0.9174\n",
      "PassengerId: 1276, Name: Wheeler, Mr. Edwin Frederick\"\", 생존 확률: 0.1868\n",
      "PassengerId: 604, Name: Torber, Mr. Ernst William, 생존 확률: 0.0431\n",
      "PassengerId: 964, Name: Nieminen, Miss. Manta Josefina, 생존 확률: 0.7039\n",
      "PassengerId: 1077, Name: Maybery, Mr. Frank Hubert, 생존 확률: 0.1393\n",
      "PassengerId: 149, Name: Navratil, Mr. Michel (\"Louis M Hoffman\"), 생존 확률: 0.1264\n",
      "PassengerId: 1062, Name: Lithman, Mr. Simon, 생존 확률: 0.0692\n",
      "PassengerId: 544, Name: Beane, Mr. Edward, 생존 확률: 0.1566\n",
      "PassengerId: 775, Name: Hocking, Mrs. Elizabeth (Eliza Needs), 생존 확률: 0.6337\n",
      "PassengerId: 1141, Name: Khalil, Mrs. Betros (Zahie Maria\" Elias)\", 생존 확률: 0.5788\n",
      "PassengerId: 699, Name: Thayer, Mr. John Borland, 생존 확률: 0.2007\n",
      "PassengerId: 63, Name: Harris, Mr. Henry Birkhardt, 생존 확률: 0.3015\n",
      "PassengerId: 605, Name: Homer, Mr. Harry (\"Mr E Haven\"), 생존 확률: 0.2988\n",
      "PassengerId: 519, Name: Angle, Mrs. William A (Florence \"Mary\" Agnes Hughes), 생존 확률: 0.8330\n",
      "PassengerId: 1139, Name: Drew, Mr. James Vivian, 생존 확률: 0.1040\n",
      "PassengerId: 1171, Name: Oxenham, Mr. Percy Thomas, 생존 확률: 0.2318\n",
      "PassengerId: 899, Name: Caldwell, Mr. Albert Francis, 생존 확률: 0.1687\n",
      "PassengerId: 685, Name: Brown, Mr. Thomas William Solomon, 생존 확률: 0.0588\n",
      "PassengerId: 398, Name: McKane, Mr. Peter David, 생존 확률: 0.1191\n",
      "PassengerId: 1117, Name: Moubarek, Mrs. George (Omine Amenia\" Alexander)\", 생존 확률: 0.5575\n",
      "PassengerId: 778, Name: Emanuel, Miss. Virginia Ethel, 생존 확률: 0.8503\n",
      "PassengerId: 411, Name: Sdycoff, Mr. Todor, 생존 확률: 0.0692\n",
      "PassengerId: 1224, Name: Thomas, Mr. Tannous, 생존 확률: 0.0494\n",
      "PassengerId: 753, Name: Vande Velde, Mr. Johannes Joseph, 생존 확률: 0.0627\n",
      "PassengerId: 204, Name: Youseff, Mr. Gerious, 생존 확률: 0.0289\n",
      "PassengerId: 474, Name: Jerwan, Mrs. Amin S (Marie Marthe Thuillard), 생존 확률: 0.8645\n",
      "PassengerId: 294, Name: Haas, Miss. Aloisia, 생존 확률: 0.7403\n",
      "PassengerId: 24, Name: Sloper, Mr. William Thompson, 생존 확률: 0.4462\n",
      "PassengerId: 50, Name: Arnold-Franchi, Mrs. Josef (Josefine Franchi), 생존 확률: 0.7521\n",
      "PassengerId: 296, Name: Lewy, Mr. Ervin G, 생존 확률: 0.3392\n",
      "PassengerId: 751, Name: Wells, Miss. Joan, 생존 확률: 0.9310\n",
      "PassengerId: 677, Name: Sawyer, Mr. Frederick Charles, 생존 확률: 0.0827\n",
      "PassengerId: 323, Name: Slayter, Miss. Hilda Mary, 생존 확률: 0.9063\n",
      "PassengerId: 556, Name: Wright, Mr. George, 생존 확률: 0.1888\n",
      "PassengerId: 828, Name: Mallet, Master. Andre, 생존 확률: 0.2713\n",
      "PassengerId: 1047, Name: Duquemin, Mr. Joseph, 생존 확률: 0.0839\n",
      "PassengerId: 135, Name: Sobey, Mr. Samuel James Hayden, 생존 확률: 0.2147\n",
      "PassengerId: 774, Name: Elias, Mr. Dibo, 생존 확률: 0.0494\n",
      "PassengerId: 486, Name: Lefebre, Miss. Jeannie, 생존 확률: 0.5532\n",
      "PassengerId: 656, Name: Hickman, Mr. Leonard Mark, 생존 확률: 0.1950\n",
      "PassengerId: 454, Name: Goldenberg, Mr. Samuel L, 생존 확률: 0.2107\n",
      "PassengerId: 827, Name: Lam, Mr. Len, 생존 확률: 0.0807\n",
      "PassengerId: 635, Name: Skoog, Miss. Mabel, 생존 확률: 0.6964\n",
      "PassengerId: 702, Name: Silverthorne, Mr. Spencer Victor, 생존 확률: 0.3783\n",
      "PassengerId: 924, Name: Dean, Mrs. Bertram (Eva Georgetta Light), 생존 확률: 0.5777\n",
      "PassengerId: 402, Name: Adams, Mr. John, 생존 확률: 0.0787\n",
      "PassengerId: 796, Name: Otter, Mr. Richard, 생존 확률: 0.1424\n",
      "PassengerId: 490, Name: Coutts, Master. Eden Leslie \"Neville\", 생존 확률: 0.1048\n",
      "PassengerId: 1007, Name: Chronopoulos, Mr. Demetrios, 생존 확률: 0.0635\n",
      "PassengerId: 1189, Name: Samaan, Mr. Hanna, 생존 확률: 0.0365\n",
      "PassengerId: 84, Name: Carrau, Mr. Francisco M, 생존 확률: 0.4560\n",
      "PassengerId: 399, Name: Pain, Dr. Alfred, 생존 확률: 0.2255\n",
      "PassengerId: 39, Name: Vander Planke, Miss. Augusta Maria, 생존 확률: 0.7168\n",
      "PassengerId: 30, Name: Todoroff, Mr. Lalio, 생존 확률: 0.0692\n",
      "PassengerId: 1113, Name: Reynolds, Mr. Harold J, 생존 확률: 0.0927\n",
      "PassengerId: 253, Name: Stead, Mr. William Thomas, 생존 확률: 0.1888\n",
      "PassengerId: 707, Name: Kelly, Mrs. Florence \"Fannie\", 생존 확률: 0.8062\n",
      "PassengerId: 645, Name: Baclini, Miss. Eugenie, 생존 확률: 0.7417\n",
      "PassengerId: 397, Name: Olsson, Miss. Elina, 생존 확률: 0.6888\n",
      "PassengerId: 142, Name: Nysten, Miss. Anna Sofia, 생존 확률: 0.7530\n",
      "PassengerId: 932, Name: Karun, Mr. Franz, 생존 확률: 0.0324\n",
      "PassengerId: 1122, Name: Sweet, Mr. George Frederick, 생존 확률: 0.3258\n",
      "PassengerId: 805, Name: Hedman, Mr. Oskar Arvid, 생존 확률: 0.0759\n",
      "PassengerId: 577, Name: Garside, Miss. Ethel, 생존 확률: 0.8601\n",
      "PassengerId: 919, Name: Daher, Mr. Shedid, 생존 확률: 0.0633\n",
      "PassengerId: 1060, Name: Cassebeer, Mrs. Henry Arthur Jr (Eleanor Genevieve Fosdick), 생존 확률: 0.9408\n",
      "PassengerId: 561, Name: Morrow, Mr. Thomas Rowan, 생존 확률: 0.0923\n",
      "PassengerId: 1222, Name: Davies, Mrs. John Morgan (Elizabeth Agnes Mary White) , 생존 확률: 0.7552\n",
      "PassengerId: 317, Name: Kantor, Mrs. Sinai (Miriam Sternin), 생존 확률: 0.8844\n",
      "PassengerId: 453, Name: Foreman, Mr. Benjamin Laventall, 생존 확률: 0.3383\n",
      "PassengerId: 412, Name: Hart, Mr. Henry, 생존 확률: 0.0921\n",
      "PassengerId: 995, Name: Johansson Palmquist, Mr. Oskar Leander, 생존 확률: 0.0787\n",
      "PassengerId: 336, Name: Denkoff, Mr. Mitto, 생존 확률: 0.0692\n",
      "PassengerId: 1216, Name: Kreuchen, Miss. Emilie, 생존 확률: 0.9685\n",
      "PassengerId: 542, Name: Andersson, Miss. Ingeborg Constanzia, 생존 확률: 0.6592\n",
      "PassengerId: 710, Name: Moubarek, Master. Halim Gonios (\"William George\"), 생존 확률: 0.0374\n",
      "PassengerId: 879, Name: Laleff, Mr. Kristo, 생존 확률: 0.0692\n",
      "PassengerId: 57, Name: Rugg, Miss. Emily, 생존 확률: 0.9064\n",
      "PassengerId: 80, Name: Dowdell, Miss. Elizabeth, 생존 확률: 0.6997\n",
      "PassengerId: 921, Name: Samaan, Mr. Elias, 생존 확률: 0.0365\n",
      "PassengerId: 771, Name: Lievens, Mr. Rene Aime, 생존 확률: 0.0844\n",
      "PassengerId: 298, Name: Allison, Miss. Helen Loraine, 생존 확률: 0.9835\n",
      "PassengerId: 551, Name: Thayer, Mr. John Borland Jr, 생존 확률: 0.4512\n",
      "PassengerId: 154, Name: van Billiard, Mr. Austin Blyler, 생존 확률: 0.0382\n",
      "PassengerId: 249, Name: Beckwith, Mr. Richard Leonard, 생존 확률: 0.3109\n",
      "PassengerId: 41, Name: Ahlin, Mrs. Johan (Johanna Persdotter Larsson), 생존 확률: 0.5739\n",
      "PassengerId: 1180, Name: Mardirosian, Mr. Sarkis, 생존 확률: 0.0494\n",
      "PassengerId: 875, Name: Abelson, Mrs. Samuel (Hannah Wizosky), 생존 확률: 0.8218\n",
      "PassengerId: 633, Name: Stahelin-Maeglin, Dr. Max, 생존 확률: 0.3246\n",
      "PassengerId: 301, Name: Kelly, Miss. Anna Katherine \"Annie Kate\", 생존 확률: 0.7591\n",
      "PassengerId: 846, Name: Abbing, Mr. Anthony, 생존 확률: 0.0460\n",
      "PassengerId: 791, Name: Keane, Mr. Andrew \"Andy\", 생존 확률: 0.0923\n",
      "PassengerId: 106, Name: Mionoff, Mr. Stoytcho, 생존 확률: 0.0737\n",
      "PassengerId: 765, Name: Eklund, Mr. Hans Linus, 생존 확률: 0.1087\n",
      "PassengerId: 352, Name: Williams-Lambert, Mr. Fletcher Fellows, 생존 확률: 0.4293\n",
      "PassengerId: 1093, Name: Danbom, Master. Gilbert Sigvard Emanuel, 생존 확률: 0.1425\n",
      "PassengerId: 1135, Name: Hyman, Mr. Abraham, 생존 확률: 0.0692\n",
      "PassengerId: 56, Name: Woolner, Mr. Hugh, 생존 확률: 0.4297\n",
      "PassengerId: 268, Name: Persson, Mr. Ernst Ulrik, 생존 확률: 0.0687\n",
      "PassengerId: 1121, Name: Hocking, Mr. Samuel James Metcalfe, 생존 확률: 0.1560\n",
      "PassengerId: 816, Name: Fry, Mr. Richard, 생존 확률: 0.4003\n",
      "PassengerId: 144, Name: Burke, Mr. Jeremiah, 생존 확률: 0.1300\n",
      "PassengerId: 443, Name: Petterson, Mr. Johan Emil, 생존 확률: 0.0687\n",
      "PassengerId: 1044, Name: Storey, Mr. Thomas, 생존 확률: 0.0265\n",
      "PassengerId: 334, Name: Vander Planke, Mr. Leo Edmondus, 생존 확률: 0.0807\n",
      "PassengerId: 502, Name: Canavan, Miss. Mary, 생존 확률: 0.8122\n",
      "PassengerId: 652, Name: Doling, Miss. Elsie, 생존 확률: 0.9076\n",
      "PassengerId: 413, Name: Minahan, Miss. Daisy E, 생존 확률: 0.9663\n",
      "PassengerId: 1046, Name: Asplund, Master. Filip Oscar, 생존 확률: 0.0514\n",
      "PassengerId: 998, Name: Buckley, Mr. Daniel, 생존 확률: 0.1225\n",
      "PassengerId: 126, Name: Nicola-Yarred, Master. Elias, 생존 확률: 0.0766\n",
      "PassengerId: 26, Name: Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson), 생존 확률: 0.4415\n",
      "PassengerId: 825, Name: Panula, Master. Urho Abraham, 생존 확률: 0.0863\n",
      "PassengerId: 460, Name: O'Connor, Mr. Maurice, 생존 확률: 0.0923\n",
      "PassengerId: 325, Name: Sage, Mr. George John Jr, 생존 확률: 0.0161\n",
      "PassengerId: 213, Name: Perkin, Mr. John Henry, 생존 확률: 0.0895\n",
      "PassengerId: 116, Name: Pekoniemi, Mr. Edvard, 생존 확률: 0.0926\n",
      "PassengerId: 851, Name: Andersson, Master. Sigvard Harald Elias, 생존 확률: 0.0695\n",
      "PassengerId: 1103, Name: Finoli, Mr. Luigi, 생존 확률: 0.0690\n",
      "PassengerId: 29, Name: O'Dwyer, Miss. Ellen \"Nellie\", 생존 확률: 0.7592\n",
      "PassengerId: 777, Name: Tobin, Mr. Roger, 생존 확률: 0.0923\n",
      "PassengerId: 548, Name: Padro y Manent, Mr. Julian, 생존 확률: 0.1389\n",
      "PassengerId: 715, Name: Greenberg, Mr. Samuel, 생존 확률: 0.0946\n",
      "PassengerId: 1120, Name: Everett, Mr. Thomas James, 생존 확률: 0.0496\n",
      "PassengerId: 162, Name: Watt, Mrs. James (Elizabeth \"Bessie\" Inglis Milne), 생존 확률: 0.8336\n",
      "PassengerId: 750, Name: Connaghton, Mr. Michael, 생존 확률: 0.0891\n",
      "PassengerId: 1218, Name: Becker, Miss. Ruth Elizabeth, 생존 확률: 0.8993\n",
      "PassengerId: 1071, Name: Compton, Mrs. Alexander Taylor (Mary Eliza Ingersoll), 생존 확률: 0.8128\n",
      "PassengerId: 322, Name: Danoff, Mr. Yoto, 생존 확률: 0.0761\n",
      "PassengerId: 5, Name: Allen, Mr. William Henry, 생존 확률: 0.0584\n",
      "PassengerId: 163, Name: Bengtsson, Mr. John Viktor, 생존 확률: 0.0787\n",
      "PassengerId: 534, Name: Peter, Mrs. Catherine (Catherine Rizk), 생존 확률: 0.5635\n",
      "PassengerId: 197, Name: Mernagh, Mr. Robert, 생존 확률: 0.0923\n",
      "PassengerId: 200, Name: Yrois, Miss. Henriette (\"Mrs Harbeck\"), 생존 확률: 0.8977\n",
      "PassengerId: 128, Name: Madsen, Mr. Fridtjof Arne, 생존 확률: 0.0838\n",
      "PassengerId: 1177, Name: Dennis, Mr. William, 생존 확률: 0.0563\n",
      "PassengerId: 8, Name: Palsson, Master. Gosta Leonard, 생존 확률: 0.0961\n",
      "PassengerId: 320, Name: Spedden, Mrs. Frederic Oakley (Margaretta Corning Stone), 생존 확률: 0.9207\n",
      "PassengerId: 640, Name: Thorneycroft, Mr. Percival, 생존 확률: 0.0600\n",
      "PassengerId: 127, Name: McMahon, Mr. Martin, 생존 확률: 0.0923\n",
      "PassengerId: 100, Name: Kantor, Mr. Sinai, 생존 확률: 0.1475\n",
      "PassengerId: 1040, Name: Crafton, Mr. John Bertram, 생존 확률: 0.4222\n",
      "PassengerId: 1132, Name: Lindstrom, Mrs. Carl Johan (Sigrid Posse), 생존 확률: 0.8666\n",
      "PassengerId: 755, Name: Herman, Mrs. Samuel (Jane Laver), 생존 확률: 0.7390\n",
      "PassengerId: 10, Name: Nasser, Mrs. Nicholas (Adele Achem), 생존 확률: 0.8857\n",
      "PassengerId: 626, Name: Sutton, Mr. Frederick, 생존 확률: 0.1974\n",
      "PassengerId: 757, Name: Carlsson, Mr. August Sigfrid, 생존 확률: 0.0737\n",
      "PassengerId: 781, Name: Ayoub, Miss. Banoura, 생존 확률: 0.7459\n",
      "PassengerId: 12, Name: Bonnell, Miss. Elizabeth, 생존 확률: 0.8926\n",
      "PassengerId: 185, Name: Kink-Heilmann, Miss. Luise Gretchen, 생존 확률: 0.8225\n",
      "PassengerId: 245, Name: Attalah, Mr. Sleiman, 생존 확률: 0.0492\n",
      "PassengerId: 1281, Name: Palsson, Master. Paul Folke, 생존 확률: 0.0844\n",
      "PassengerId: 800, Name: Van Impe, Mrs. Jean Baptiste (Rosalie Paula Govaert), 생존 확률: 0.6384\n",
      "PassengerId: 910, Name: Ilmakangas, Miss. Ida Livija, 생존 확률: 0.6804\n",
      "PassengerId: 850, Name: Goldenberg, Mrs. Samuel L (Edwiga Grabowska), 생존 확률: 0.9423\n",
      "PassengerId: 196, Name: Lurette, Miss. Elise, 생존 확률: 0.8975\n",
      "PassengerId: 1203, Name: Vartanian, Mr. David, 생존 확률: 0.0643\n",
      "PassengerId: 23, Name: McGowan, Miss. Anna \"Annie\", 생존 확률: 0.8428\n",
      "PassengerId: 776, Name: Myhrman, Mr. Pehr Fabian Oliver Malkolm, 생존 확률: 0.1020\n",
      "PassengerId: 299, Name: Saalfeld, Mr. Adolphe, 생존 확률: 0.4255\n",
      "PassengerId: 342, Name: Fortune, Miss. Alice Elizabeth, 생존 확률: 0.9651\n",
      "PassengerId: 975, Name: Demetri, Mr. Marinko, 생존 확률: 0.0692\n",
      "PassengerId: 240, Name: Hunt, Mr. George Henry, 생존 확률: 0.1702\n",
      "PassengerId: 900, Name: Abrahim, Mrs. Joseph (Sophie Halaut Easu), 생존 확률: 0.7106\n",
      "PassengerId: 913, Name: Olsen, Master. Artur Karl, 생존 확률: 0.1185\n",
      "PassengerId: 376, Name: Meyer, Mrs. Edgar Joseph (Leila Saks), 생존 확률: 0.9410\n",
      "PassengerId: 1181, Name: Ford, Mr. Arthur, 생존 확률: 0.0693\n",
      "PassengerId: 963, Name: Minkoff, Mr. Lazar, 생존 확률: 0.0926\n",
      "PassengerId: 608, Name: Daniel, Mr. Robert Williams, 생존 확률: 0.4508\n",
      "PassengerId: 304, Name: Keane, Miss. Nora A, 생존 확률: 0.9067\n",
      "PassengerId: 1158, Name: Chisholm, Mr. Roderick Robert Crispin, 생존 확률: 0.4003\n",
      "PassengerId: 244, Name: Maenpaa, Mr. Matti Alexanteri, 생존 확률: 0.0895\n",
      "PassengerId: 721, Name: Harper, Miss. Annie Jessie \"Nina\", 생존 확률: 0.9397\n",
      "PassengerId: 917, Name: Robins, Mr. Alexander A, 생존 확률: 0.0300\n",
      "PassengerId: 1057, Name: Kink-Heilmann, Mrs. Anton (Luise Heilmann), 생존 확률: 0.6690\n",
      "PassengerId: 330, Name: Hippach, Miss. Jean Gertrude, 생존 확률: 0.9619\n",
      "PassengerId: 221, Name: Sunderland, Mr. Victor Francis, 생존 확률: 0.1088\n",
      "PassengerId: 912, Name: Rothschild, Mr. Martin, 생존 확률: 0.1631\n",
      "PassengerId: 920, Name: Brady, Mr. John Bertram, 생존 확률: 0.3326\n",
      "PassengerId: 72, Name: Goodwin, Miss. Lillian Amy, 생존 확률: 0.5700\n",
      "PassengerId: 158, Name: Corn, Mr. Harry, 생존 확률: 0.0690\n",
      "PassengerId: 615, Name: Brocklebank, Mr. William Alfred, 생존 확률: 0.0584\n",
      "PassengerId: 1232, Name: Fillbrook, Mr. Joseph Charles, 생존 확률: 0.2581\n",
      "PassengerId: 1163, Name: Fox, Mr. Patrick, 생존 확률: 0.0923\n",
      "PassengerId: 263, Name: Taussig, Mr. Emil, 생존 확률: 0.2247\n",
      "PassengerId: 89, Name: Fortune, Miss. Mabel Helen, 생존 확률: 0.9662\n",
      "PassengerId: 938, Name: Chevre, Mr. Paul Romaine, 생존 확률: 0.2317\n",
      "PassengerId: 746, Name: Crosby, Capt. Edward Gifford, 생존 확률: 0.1291\n",
      "PassengerId: 801, Name: Ponesell, Mr. Martin, 생존 확률: 0.1656\n",
      "PassengerId: 512, Name: Webber, Mr. James, 생존 확률: 0.0693\n",
      "PassengerId: 62, Name: Icard, Miss. Amelie, 생존 확률: 0.9531\n",
      "PassengerId: 262, Name: Asplund, Master. Edvin Rojj Felix, 생존 확률: 0.0718\n",
      "PassengerId: 210, Name: Blank, Mr. Henry, 생존 확률: 0.2658\n",
      "PassengerId: 307, Name: Fleming, Miss. Margaret, 생존 확률: 0.9548\n",
      "PassengerId: 1136, Name: Johnston, Master. William Arthur Willie\"\", 생존 확률: 0.0475\n",
      "PassengerId: 887, Name: Montvila, Rev. Juozas, 생존 확률: 0.2030\n",
      "PassengerId: 371, Name: Harder, Mr. George Achilles, 생존 확률: 0.3589\n",
      "PassengerId: 442, Name: Hampe, Mr. Leon, 생존 확률: 0.0961\n",
      "PassengerId: 1260, Name: Gibson, Mrs. Leonard (Pauline C Boeson), 생존 확률: 0.9002\n",
      "PassengerId: 60, Name: Goodwin, Master. William Frederick, 생존 확률: 0.0487\n",
      "PassengerId: 625, Name: Bowen, Mr. David John \"Dai\", 생존 확률: 0.0950\n",
      "PassengerId: 1131, Name: Douglas, Mrs. Walter Donald (Mahala Dutton), 생존 확률: 0.9009\n",
      "PassengerId: 64, Name: Skoog, Master. Harald, 생존 확률: 0.0813\n",
      "PassengerId: 588, Name: Frolicher-Stehli, Mr. Maxmillian, 생존 확률: 0.1322\n",
      "PassengerId: 744, Name: McNamee, Mr. Neal, 생존 확률: 0.0729\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1️⃣ 데이터 불러오기\n",
    "df = pd.read_csv(\"titanic1309.csv\")\n",
    "\n",
    "# 2️⃣ 사용할 열 선택\n",
    "X = df[['PassengerId', 'Name', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = df['Survived']\n",
    "\n",
    "# 3️⃣ 결측치 처리 (수치형: 평균으로, 범주형: 가장 많은 값으로)\n",
    "X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
    "X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
    "X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# 4️⃣ 범주형을 숫자로 변환 (원-핫 인코딩)\n",
    "X_model = X.drop(['PassengerId', 'Name'], axis=1)\n",
    "X_model = pd.get_dummies(X_model, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# 5️⃣ 수치형 변수 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_model[['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']] = scaler.fit_transform(\n",
    "    X_model[['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']]\n",
    ")\n",
    "\n",
    "# 6️⃣ 학습용, 테스트용 데이터셋으로 나누기 (PassengerId, Name 포함해서 같이 나누기)\n",
    "X_train_model, X_test_model, X_train_meta, X_test_meta, y_train, y_test = train_test_split(\n",
    "    X_model, X[['PassengerId', 'Name']], y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7️⃣ TensorFlow 모델\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train_model.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_model, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 8️⃣ 테스트셋 정확도 평가\n",
    "test_loss, test_acc = model.evaluate(X_test_model, y_test, verbose=0)\n",
    "print(f\"[TensorFlow] Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 9️⃣ 테스트셋 각 샘플별 생존 확률 출력 (PassengerId, Name 포함)\n",
    "y_pred_probs = model.predict(X_test_model)\n",
    "\n",
    "# 0~1 사이의 생존 확률과 PassengerId, Name 함께 출력\n",
    "print(\"\\n🎯 테스트셋 각 샘플별 생존 확률:\")\n",
    "for idx in range(len(y_pred_probs)):\n",
    "    passenger_id = X_test_meta.iloc[idx]['PassengerId']\n",
    "    passenger_name = X_test_meta.iloc[idx]['Name']\n",
    "    prob = y_pred_probs[idx][0]\n",
    "    print(f\"PassengerId: {passenger_id}, Name: {passenger_name}, 생존 확률: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28211f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/1830346097.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/1830346097.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/1830346097.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/1830346097.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/1830346097.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/1830346097.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
      "/opt/miniconda3/envs/VDLP/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4847 - loss: 0.8372 - val_accuracy: 0.6333 - val_loss: 0.6874\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6661 - loss: 0.6697 - val_accuracy: 0.7000 - val_loss: 0.5971\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6860 - loss: 0.6256 - val_accuracy: 0.7238 - val_loss: 0.5522\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6842 - loss: 0.5914 - val_accuracy: 0.7571 - val_loss: 0.5193\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7059 - loss: 0.5799 - val_accuracy: 0.7810 - val_loss: 0.4964\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7379 - loss: 0.5249 - val_accuracy: 0.7905 - val_loss: 0.4770\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7515 - loss: 0.5192 - val_accuracy: 0.8095 - val_loss: 0.4605\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7418 - loss: 0.5287 - val_accuracy: 0.8190 - val_loss: 0.4468\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7483 - loss: 0.5250 - val_accuracy: 0.8143 - val_loss: 0.4347\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7876 - loss: 0.4724 - val_accuracy: 0.8429 - val_loss: 0.4231\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7882 - loss: 0.4704 - val_accuracy: 0.8524 - val_loss: 0.4163\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7968 - loss: 0.4542 - val_accuracy: 0.8667 - val_loss: 0.4087\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8156 - loss: 0.4499 - val_accuracy: 0.8762 - val_loss: 0.4056\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8215 - loss: 0.4464 - val_accuracy: 0.8762 - val_loss: 0.3984\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8183 - loss: 0.4501 - val_accuracy: 0.8714 - val_loss: 0.3937\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8238 - loss: 0.4441 - val_accuracy: 0.8714 - val_loss: 0.3904\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8425 - loss: 0.4007 - val_accuracy: 0.8762 - val_loss: 0.3844\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8390 - loss: 0.4219 - val_accuracy: 0.8762 - val_loss: 0.3823\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8273 - loss: 0.4319 - val_accuracy: 0.8714 - val_loss: 0.3798\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8300 - loss: 0.4183 - val_accuracy: 0.8762 - val_loss: 0.3774\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8517 - loss: 0.4090 - val_accuracy: 0.8714 - val_loss: 0.3745\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8381 - loss: 0.4040 - val_accuracy: 0.8714 - val_loss: 0.3735\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8302 - loss: 0.4156 - val_accuracy: 0.8714 - val_loss: 0.3740\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8613 - loss: 0.3889 - val_accuracy: 0.8714 - val_loss: 0.3727\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8393 - loss: 0.4023 - val_accuracy: 0.8714 - val_loss: 0.3722\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8413 - loss: 0.4235 - val_accuracy: 0.8714 - val_loss: 0.3721\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8247 - loss: 0.4014 - val_accuracy: 0.8714 - val_loss: 0.3719\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8447 - loss: 0.4049 - val_accuracy: 0.8714 - val_loss: 0.3704\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8513 - loss: 0.3922 - val_accuracy: 0.8714 - val_loss: 0.3706\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8428 - loss: 0.3982 - val_accuracy: 0.8714 - val_loss: 0.3679\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8181 - loss: 0.4437 - val_accuracy: 0.8714 - val_loss: 0.3695\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8544 - loss: 0.3886 - val_accuracy: 0.8714 - val_loss: 0.3682\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8539 - loss: 0.3983 - val_accuracy: 0.8714 - val_loss: 0.3681\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8433 - loss: 0.4167 - val_accuracy: 0.8762 - val_loss: 0.3670\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8398 - loss: 0.4078 - val_accuracy: 0.8714 - val_loss: 0.3709\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8697 - loss: 0.3627 - val_accuracy: 0.8762 - val_loss: 0.3717\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8557 - loss: 0.3734 - val_accuracy: 0.8810 - val_loss: 0.3718\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8236 - loss: 0.3777 - val_accuracy: 0.8762 - val_loss: 0.3691\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8456 - loss: 0.3879 - val_accuracy: 0.8762 - val_loss: 0.3692\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8557 - loss: 0.3787 - val_accuracy: 0.8762 - val_loss: 0.3701\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8395 - loss: 0.3896 - val_accuracy: 0.8714 - val_loss: 0.3716\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8416 - loss: 0.4007 - val_accuracy: 0.8714 - val_loss: 0.3705\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8652 - loss: 0.3692 - val_accuracy: 0.8714 - val_loss: 0.3743\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8488 - loss: 0.3956 - val_accuracy: 0.8714 - val_loss: 0.3749\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8329 - loss: 0.3988 - val_accuracy: 0.8714 - val_loss: 0.3715\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8280 - loss: 0.4422 - val_accuracy: 0.8714 - val_loss: 0.3713\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8234 - loss: 0.4169 - val_accuracy: 0.8714 - val_loss: 0.3686\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8384 - loss: 0.4030 - val_accuracy: 0.8714 - val_loss: 0.3696\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8466 - loss: 0.3908 - val_accuracy: 0.8762 - val_loss: 0.3706\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8394 - loss: 0.4230 - val_accuracy: 0.8762 - val_loss: 0.3687\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "🎯 전체 입력 데이터셋 생존 확률 (앞 10개):\n",
      "PassengerId: 1, Name: Braund, Mr. Owen Harris, 생존 확률: 0.0790\n",
      "PassengerId: 2, Name: Cumings, Mrs. John Bradley (Florence Briggs Thayer), 생존 확률: 0.8943\n",
      "PassengerId: 3, Name: Heikkinen, Miss. Laina, 생존 확률: 0.7334\n",
      "PassengerId: 4, Name: Futrelle, Mrs. Jacques Heath (Lily May Peel), 생존 확률: 0.9330\n",
      "PassengerId: 5, Name: Allen, Mr. William Henry, 생존 확률: 0.0602\n",
      "PassengerId: 6, Name: Moran, Mr. James, 생존 확률: 0.0827\n",
      "PassengerId: 7, Name: McCarthy, Mr. Timothy J, 생존 확률: 0.2107\n",
      "PassengerId: 8, Name: Palsson, Master. Gosta Leonard, 생존 확률: 0.1074\n",
      "PassengerId: 9, Name: Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg), 생존 확률: 0.6961\n",
      "PassengerId: 10, Name: Nasser, Mrs. Nicholas (Adele Achem), 생존 확률: 0.8661\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# 1️⃣ 데이터 불러오기\n",
    "df = pd.read_csv(\"titanic1309.csv\")\n",
    "\n",
    "# 2️⃣ 사용할 열 선택\n",
    "X = df[['PassengerId', 'Name', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = df['Survived']\n",
    "\n",
    "# 3️⃣ 결측치 처리\n",
    "X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
    "X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
    "X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# 4️⃣ 원-핫 인코딩 (PassengerId, Name 제외)\n",
    "X_model = X.drop(['PassengerId', 'Name'], axis=1)\n",
    "X_model = pd.get_dummies(X_model, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# 5️⃣ 수치형 변수 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_model[['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']] = scaler.fit_transform(\n",
    "    X_model[['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']]\n",
    ")\n",
    "\n",
    "# 6️⃣ 학습용, 테스트용 데이터셋으로 나누기\n",
    "X_train_model, X_test_model, X_train_meta, X_test_meta, y_train, y_test = train_test_split(\n",
    "    X_model, X[['PassengerId', 'Name']], y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7️⃣ 모델 정의 및 학습\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train_model.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_model, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 8️⃣ 전체 입력 데이터셋의 생존 확률 구하기\n",
    "y_pred_probs_all = model.predict(X_model)\n",
    "\n",
    "# 9️⃣ 원본 데이터와 생존 확률 합치기\n",
    "df['Survival_Probability'] = y_pred_probs_all\n",
    "\n",
    "# 🔟 결과 출력 (앞 10개만 예시)\n",
    "print(\"\\n🎯 전체 입력 데이터셋 생존 확률 (앞 10개):\")\n",
    "for idx, row in df.head(10).iterrows():\n",
    "    print(f\"PassengerId: {row['PassengerId']}, Name: {row['Name']}, 생존 확률: {row['Survival_Probability']:.4f}\")\n",
    "\n",
    "# 1️⃣1️⃣ (선택) CSV 파일로 저장\n",
    "# df[['PassengerId', 'Name', 'Survival_Probability']].to_csv('titanic_survival_probabilities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ed4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass     Sex   Age  SibSp  Parch     Fare Embarked  Pclass_Survival_Prob  \\\n",
      "0       3    male  22.0      1      0   7.2500        S              0.271143   \n",
      "1       1  female  38.0      1      0  71.2833        C              0.570291   \n",
      "2       3  female  26.0      0      0   7.9250        S              0.271143   \n",
      "3       1  female  35.0      1      0  53.1000        S              0.570291   \n",
      "4       3    male  35.0      0      0   8.0500        S              0.271143   \n",
      "\n",
      "   Sex_Survival_Prob  Age_Survival_Prob  SibSp_Survival_Prob  \\\n",
      "0           0.133316           0.391650             0.377935   \n",
      "1           0.819024           0.362310             0.377935   \n",
      "2           0.819024           0.384232             0.376837   \n",
      "3           0.819024           0.367741             0.377935   \n",
      "4           0.133316           0.367741             0.376837   \n",
      "\n",
      "   Parch_Survival_Prob  Fare_Survival_Prob  Embarked_Survival_Prob  \n",
      "0             0.353735            0.311966                0.335817  \n",
      "1             0.353735            0.485261                0.489335  \n",
      "2             0.353735            0.313625                0.335817  \n",
      "3             0.353735            0.433686                0.335817  \n",
      "4             0.353735            0.313933                0.335817  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer   # 결측치 처리용\n",
    "\n",
    "# 1️⃣ 데이터 불러오기 ─ 파일 경로를 자신의 CSV 경로로 바꿔 주세요\n",
    "df = pd.read_csv(\"titanic1309.csv\")\n",
    "\n",
    "# 2️⃣ 특성 리스트\n",
    "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# 3️⃣ 범주형으로 다룰 열 지정\n",
    "categorical_feats = [\"Pclass\", \"Sex\", \"Embarked\"]   # Pclass도 범주처럼 취급\n",
    "\n",
    "def compute_survival_prob(feature: str, data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    단일 feature만 사용해 생존 확률을 계산한 뒤,\n",
    "    (행 수,) 모양의 시리즈를 반환한다.\n",
    "    \"\"\"\n",
    "    X = data[[feature]].copy()\n",
    "    y = data[\"Survived\"]\n",
    "\n",
    "    # ── 전처리 파이프라인 구성 ─────────────────────────────────── #\n",
    "    if feature in categorical_feats:                          # ▸ 범주형\n",
    "        preprocessor = ColumnTransformer(\n",
    "            [(\"cat\",\n",
    "              Pipeline([\n",
    "                  (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                  (\"ohe\", OneHotEncoder(drop=\"first\"))\n",
    "              ]),\n",
    "              [feature])],\n",
    "            remainder=\"passthrough\"\n",
    "        )\n",
    "    else:                                                     # ▸ 수치형\n",
    "        preprocessor = ColumnTransformer(\n",
    "            [(\"num\",\n",
    "              Pipeline([\n",
    "                  (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "                  (\"scale\", StandardScaler())\n",
    "              ]),\n",
    "              [feature])],\n",
    "            remainder=\"passthrough\"\n",
    "        )\n",
    "    # ───────────────────────────────────────────────────────── #\n",
    "\n",
    "    model = Pipeline(\n",
    "        [(\"prep\", preprocessor),\n",
    "         (\"clf\", LogisticRegression(max_iter=1000))]\n",
    "    )\n",
    "\n",
    "    model.fit(X, y)\n",
    "    prob = model.predict_proba(X)[:, 1]   # 생존 클래스(1)의 확률\n",
    "    return pd.Series(prob, name=f\"{feature}_Survival_Prob\")\n",
    "\n",
    "# 4️⃣ 각 특성별 생존 확률 계산 & 원본 데이터프레임에 추가\n",
    "for feat in features:\n",
    "    df[f\"{feat}_Survival_Prob\"] = compute_survival_prob(feat, df)\n",
    "\n",
    "# 5️⃣ 확인\n",
    "print(df[[*features, *[f\"{f}_Survival_Prob\" for f in features]]].head())\n",
    "\n",
    "# 6️⃣ 필요하다면 저장\n",
    "# df.to_csv(\"titanic_with_featurewise_probs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f97244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/2698093281.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
      "/var/folders/tk/ct0j8j1d0y1ft37qgdzljbzh0000gn/T/ipykernel_2158/2698093281.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mEmbarked\u001b[39m\u001b[33m'\u001b[39m].fillna(df[\u001b[33m'\u001b[39m\u001b[33mEmbarked\u001b[39m\u001b[33m'\u001b[39m].mode()[\u001b[32m0\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 4️⃣ 범주형 변수 원-핫 인코딩\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m encoder = \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfirst\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m categorical = [\u001b[33m'\u001b[39m\u001b[33mPclass\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSex\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mEmbarked\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     21\u001b[39m encoded = encoder.fit_transform(df[categorical])\n",
      "\u001b[31mTypeError\u001b[39m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1️⃣ 데이터 로드\n",
    "df = pd.read_csv('titanic1309.csv')\n",
    "\n",
    "# 2️⃣ 필요한 열 선택\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# 3️⃣ 전처리: 결측치 채우기\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# 4️⃣ 범주형 변수 원-핫 인코딩\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "categorical = ['Pclass', 'Sex', 'Embarked']\n",
    "encoded = encoder.fit_transform(df[categorical])\n",
    "\n",
    "# 인코딩된 열 이름 다시 생성\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical))\n",
    "df = pd.concat([df.drop(columns=categorical), encoded_df], axis=1)\n",
    "\n",
    "# 5️⃣ 표준화 (스케일링)\n",
    "scaler = StandardScaler()\n",
    "numerical = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "df[numerical] = scaler.fit_transform(df[numerical])\n",
    "\n",
    "# 6️⃣ 각 Feature별로 개별 모델 생성 & 예측\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "# 피처별 생존 확률 예측 저장\n",
    "feature_probabilities = {}\n",
    "\n",
    "for feature in X.columns:\n",
    "    # 개별 Feature만 사용\n",
    "    X_feature = X[[feature]]\n",
    "    \n",
    "    # 학습/테스트 분리\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 로지스틱 회귀 모델 학습\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 확률 예측\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]  # 생존 확률 (1의 확률)\n",
    "    \n",
    "    # 평균 생존 확률 계산\n",
    "    mean_prob = y_prob.mean()\n",
    "    feature_probabilities[feature] = mean_prob\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n🔎 각 Feature별 평균 생존 확률:\")\n",
    "for feature, prob in feature_probabilities.items():\n",
    "    print(f\"{feature}: {prob:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
